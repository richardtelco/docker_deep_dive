# How can we scale our containers?
# 2 components: 
#  - swarm cluster (1 or more docker 'nodes' - first one is a 'manager' node)
#  - orchestration engine
# docker swarm features
#  - encrypted traffic between nodes and secure join tokens between manager and worker nodes
#  - an API
#  - a manifest file so you can 'declare' your swarm dimensions/configs/componentry
#  - methods for 
#    - update
#    - rollback
#    - scale 
# docker swarm became a part of docker in v1.12
# Manager nodes
#   - manages state of cluster
#   - sends tasks to workers nodes
# managernode:/etcd/ is where the docker swarm state is kept (and is kept very up to date)
# docker swarm is arguably more secure than docker on its own so you may want to run docker using swarm all the time - even if its only 1 container you want!
# Atomic unit of scheduling a docker swarm is a "swarm service"
# Docker swarm services adds to a docker container service..:
#  - scaling
#  - rolling updates
#  - rollback
# When a docker container is "wrapped in a docker swarm service", its usually called a "task" or a "replica". 

## Docker swarm nodes 
# imagine setting up a docker swarm with 3 nodes (virtual machines) - one allocated to be manager and the other two allocated as worker nodes  
# all three are running docker
# Go to manager node and run:
docker swarm init --advertise-addr 172.28.28.103        # that IP address is the one the manager node will be listening for API calls on....
#                                                         ...as well as the one other nodes will use to talk to it. Also the 'overlay' network
# Next we get the docker command to run on the worker nodes to "join the manager's swarm"
# We need to use the PRIVATE IP to join ,and generally the port 2377.
# It'll look something like this: (might need to precede with "sudo")
docker swarm join --token SWMTKN-1-hsefkvsbufsbgvkuybgscknr 172.28.28.103:2377
# if you can't see it, execute the following
docker swarm join-token worker      # this gives the command you need to run on a node that we want to become a worker node 
# When done, go back to swarm manager node and check what's joined your swarm...
docker node ls 
# This should now list the two other nodes
# Next we can get info about a specific node
docker node inspect hostname.mynetwork.com        # could also use nodeid rather than hostname
# This shows things like the docker engine version its running, the platform its running on, the OS, the cert its using and the node status (eg active)
# Workers can be promoted by the manager.... and demoted again later
docker node promote hostname2.mynetwork.com     # this is now a manager node 
docker node demote hostname2.mynetwork.com      # this is back to being a worker node 
docker node rm -f hostname2.mynetwork.com       # this removes the node from the swarm from the managers' perspective....
#                                                 ...but the hode itself doesn't yet realise - so we need to go to it and tell it to leave
ssh user@hostname2.mynetwork.com
docker swarm leave  
# Now its just back to being a lonesome docker host. If we want it back in the swarm we need to do a docker swarm join again.
# Note that docker nodes can join the swarm as managers directly (rather than as workers, then get promoted)

## Docker swarm services
# when you want to deploy an applicaiton on docker swarm, you deploy it as a 'service'...
# to create a 'service', run something like the following:
docker service create -d --name nginx_service -p 8080:80 --replicas 2 nginx:latest
# NOTE the critical new tag - "--replicas". This time the docker service "mode" is called "replicated" - but otherwise it looks like a normal docker service. However I think the "mode=replicated" means it's using docker swarm 
# the service will create 'tasks' running on available worker nodes which themselves create containers on those worker nodes to be part of the "swarm"...
# The service runs 'n' "replicas" (of the docker container, as docker containers) on the available worker node population
# It will also load balance the traffic to these containers
# As with normal containers, we can inspect the service
docker service inspect nginx_service 
# You can observe the network on which the docket hosts communicate by grepping for "networkid" from teh output of the service inspect
# Then you can look up that networkid in the docker network ls:
docker network ls | grep 871346r38742680       # where 871346r38742680 is the networkid quoted in the service
# This will show that the DRIVER=overlay and SCOPE=swarm
# you can also get the logs of the service
docker service logs nginx_service
# We can list the tasks of a service
docker service ps nginx_service
# We can also scale up the number of tasks in the service whilst its running
docker service scale nginx_service=3
# after this we can lists the tastks again with docker service ps
# We can also update other things on the service whilst its running. There are many. See what they are by:
docker service update -h     # -h flag means "help" - and lists all the options
# for example we can update environment variables, add or remove mounts etc.
# To hit the service, we can't hit localhost - we have to hit either the public or private IPs...

## Docker swarm networks
# The overlay network used by docker swarm is an "overlay" ontop of the networks between the hosts and enables nodes to communciate with one another
# note that this creates a network with a network driver called 'overlay'. This is different from the network that was previously created when mulitple containers were created on the same host machine. When those containers were isolated on a docker network on the one host machine, that network driver was called 'bridge' and was just bridging containers on the same host using a 'network'.
# This overlay network is usually called "ingress" (the default one if you don't specify something else)
# However you CAN specify your own "overlay" network, and ensure it's encrypted on the network the hosts are being isolated on, with the encrypted flag
# Start by creating the network
docker network create \
  --driver overlay \
  --opt encrypted
  encrypted_overlay
# its easier to create the network first, then specify it when creating the swarm service
# This is very much like the above docker service create line (line55) but with the network flag:
docker service create -d --name nginx_overlay --network encrypted_network -p 8080:80 --replicas 2 nginx:latest

# DOCKER VOLUMES with swarm
# Volumes are local to your docker host - so although you can create volumes on the nodes all tasks are hosted on...
# ... the data on them will be different. This doesn't really work so....
# need to use "plugins".
# And example plugin might be to have docker write its logs to a splunk index. To isntall the docker splunk plugin, we can:
docker plugin install splunk/docker-logging-plugin splunk/docker-logging-plugin
# to get a list of all the plugins you;ve got installed
docker plugins ls
# you can disable them - but you need to use the pluginid rather than that pluginname - which you got from docker plugin ls 
# docker plugin disable [PLUGINID]
# To use a shared docker volume, on something non-local, we can use a docker plugin called rexray/ebs (rexray is the company and plugin...
# ... ebs is Elastic Block Store - an amazon web services product that provide block storage)
# you;d install it in the following way:
docker plugin install rexray/ebs EBS_ACCESSKEY=w7463trf3dh4 EBS_SECRETKEY=8764crf387f34
# If you want to supply the region, that could be done
# First create a normal local volume on the master/manager node
docker volume create -d local portainer_data        # -d is the same as --driver (storage "Driver" for the 'volume')
# This volume won't appear on other hosts running containers that form part of the swarm. 
# When we create a volume used by the (swarm) service, we can't use the volume flag (cos tahts local only), so we have to use "mount" instead.
docker service create \
--name portainer \
--publish 8000:9000 \
--constraint 'node.role == manager' \
--mount type=volume,src=portainer_data,dst=/data \
--mount type=bind,src=/var/run.docker.sock,dst=/var/run/docker.sock \
portainer/portainer \
-H unix:///var/run.docker.sock 
# the portainer service you created will be able tos ee the swarm service you just created!

# DEPLOYING STACKS in docker swarm
# Stacks lets you deploy out a complete application in your swarm environment
# we use a compose file to define the stack
# So think of using docker compose, but this time with docker swarm rather than just docker in single node form 
# start by creating a config file (.yml)
# In this file we'd do various things like
#  - the version of the docker compose file (minimum v3 for docker swarm)
#  - the docker container images that the applicaiton needs to run
#  - container run flags when that container need to be run
#  - volumes the containers need to connect to / mount 
#  - dependencies - if containers need to load in specific orders
# note that the "deploy" attribute of a docker compose file is ONLY interpretted by docker swarm - it means nothing to docker 
# container info like "how many replicas" are required
# its alsways worth looking at the docker files for the images you;re going to be using. 
# When we have completed our docker compose file we need to deploy the stack. We do this something like this:
docker stack deploy --compose-file docker-compose.yml mystack
# Then we can see what stacks we have deployed and running:
docker stack ls
# we can also ps to list the task replicas across the service (remember tasks run individual containers)
docker stack ps mystack 
# removing stacks is fairly expected
docker stack rm mystack
# below is an example docker compose file for deploying a stack for wordpress
version: '3'

services:
   db:
     image: mysql:5.7
     volumes:
       - db_data:/var/lib/mysql
     networks:
       mysql_internal:
         aliases: ["db"]
     environment:
       MYSQL_ROOT_PASSWORD: P4ssw0rd0!
       MYSQL_DATABASE: wordpress
       MYSQL_USER: wordpress
       MYSQL_PASSWORD: P4ssw0rd0!

   blog:
     depends_on:
       - db
     image: wordpress:latest
     networks:
       mysql_internal:
       wordpress_public:
     ports:
       - "80:80"
     environment:
       WORDPRESS_DB_HOST: db:3306
       WORDPRESS_DB_USER: wordpress
       WORDPRESS_DB_PASSWORD: P4ssw0rd0!

volumes:
    db_data:
networks:
  mysql_internal:
    internal: true
  wordpress_public:

## Docker Security
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
